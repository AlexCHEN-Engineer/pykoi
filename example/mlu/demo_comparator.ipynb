{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone from github\n",
    "# git clone https://github.com/CambioML/pykoi.git /home/ec2-user/SageMaker/pykoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1:\n",
    "# use github code as source\n",
    "import sys\n",
    "sys.path.append(\"/home/ec2-user/SageMaker/pykoi\")\n",
    "!pip install poetry\n",
    "!pip install -q nest_asyncio\n",
    "!poetry install --no-root\n",
    "!pip3 install torch --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Option 2:\n",
    "# # pip install pykoi into site-packages\n",
    "# !pip install -q nest_asyncio\n",
    "# !pip install pykoi\n",
    "# !pip3 install torch --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pykoi\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"create model 1...\")\n",
    "hf_model_1 = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"tiiuae/falcon-rw-1b\",\n",
    "    trust_remote_code=True,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"create tokenizer 1...\")\n",
    "hf_tokenizer_1 = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"tiiuae/falcon-rw-1b\",\n",
    "    trust_remote_code=True,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "###################################################################################\n",
    "# Creating a Huggingface model tiiuae/falcon-7b (EC2 g4.2xlarge with 100GB space) #\n",
    "###################################################################################\n",
    "print(\"create model 2...\")\n",
    "hf_model_2 = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"tiiuae/falcon-7b\",\n",
    "    trust_remote_code=True,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"create tokenizer 2...\")\n",
    "hf_tokenizer_2 = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"tiiuae/falcon-7b\",\n",
    "    trust_remote_code=True,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "#########################################################################################\n",
    "# Creating a Huggingface model databricks/dolly-v2-3b (EC2 g4.2xlarge with 100GB space) #\n",
    "#########################################################################################\n",
    "print(\"create model 3...\")\n",
    "hf_model_3 = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"databricks/dolly-v2-3b\",\n",
    "    trust_remote_code=True,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"create tokenizer 3...\")\n",
    "hf_tokenizer_3 = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"databricks/dolly-v2-3b\",\n",
    "    trust_remote_code=True,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in a list of models to compare\n",
    "model_name = [\"falcon-rw-1b\", \"falcon-7b\", \"dolly-v2-3b\"]\n",
    "models = [hf_model_1, hf_model_2, hf_model_3]\n",
    "tokenizers = [hf_tokenizer_1, hf_tokenizer_2, hf_tokenizer_3]\n",
    "\n",
    "models_list = [\n",
    "    pykoi.chat.llm.huggingface.HuggingfaceModel.create(\n",
    "        model=model, tokenizer=tokenizer, name=name, max_length=100\n",
    "    )\n",
    "    for model, tokenizer, name in zip(models, tokenizers, model_name)\n",
    "]\n",
    "chatbot_comparator = pykoi.Compare(models=models_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = pykoi.Application()\n",
    "app.add_component(chatbot_comparator)\n",
    "app.run()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
