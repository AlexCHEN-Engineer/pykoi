{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# # Add the root folder to the module search path\n",
    "# # Get the current directory\n",
    "# current_directory = os.getcwd()\n",
    "\n",
    "# # Move two levels up (go to the parent directory of the parent directory)\n",
    "# # two_levels_up_directory = os.path.dirname(os.path.dirname(current_directory))\n",
    "# one_levels_up_directory = os.path.dirname(current_directory)\n",
    "\n",
    "# print(one_levels_up_directory)\n",
    "\n",
    "# sys.path.append(two_levels_up_directory)\n",
    "sys.path.append(\"/home/ubuntu/git/pykoi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerate Config (Once per machine)\n",
    "reference: https://huggingface.co/docs/accelerate/basic_tutorials/notebook\n",
    "\n",
    "\n",
    "```\n",
    "(pykoi) $ accelerate config\n",
    "----------------------------------------------------------------------------------In which compute environment are you running?\n",
    "This machine                                                                      \n",
    "----------------------------------------------------------------------------------Which type of machine are you using?                                              \n",
    "multi-CPU                                                                         \n",
    "How many different machines will you use (use more than 1 for multi-node training)? [1]: 1                                                                          \n",
    "Do you want to use Intel PyTorch Extension (IPEX) to speed up training on CPU? [yes/NO]:No                                                                          \n",
    "Do you wish to optimize your script with torch dynamo?[yes/NO]:No                 \n",
    "How many CPU(s) should be used for distributed training? [1]:8                    \n",
    "----------------------------------------------------------------------------------Do you wish to use FP16 or BF16 (mixed precision)?\n",
    "fp16                                                                              \n",
    "accelerate configuration saved at /home/ubuntu/.cache/huggingface/accelerate/default_config.yaml   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pykoipypi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pykoi\n",
    "from pykoi import RLHFConfig\n",
    "\n",
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from pykoi.db.qa_database import QuestionAnswerDatabase\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, TaskType, get_peft_model\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import (Adafactor, AutoModelForCausalLM,\n",
    "                          AutoModelForSequenceClassification, AutoTokenizer,\n",
    "                          Trainer, TrainerCallback, TrainingArguments, logging,\n",
    "                          pipeline, set_seed)\n",
    "from transformers.utils import PushToHubMixin\n",
    "from trl import (AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer,\n",
    "                 SFTTrainer)\n",
    "from trl.core import LengthSampler\n",
    "from trl.trainer.utils import ConstantLengthDataset, PeftSavingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_CSV_HEADER_ID = 'ID'\n",
    "QA_CSV_HEADER_QUESTION = 'Question'\n",
    "QA_CSV_HEADER_ANSWER = 'Answer'\n",
    "QA_CSV_HEADER_VOTE_STATUS = 'Vote Status'\n",
    "QA_CSV_HEADER_TIMESTAMPS = 'Timestamp'\n",
    "QA_CSV_HEADER = (\n",
    "    QA_CSV_HEADER_ID,\n",
    "    QA_CSV_HEADER_QUESTION,\n",
    "    QA_CSV_HEADER_ANSWER,\n",
    "    QA_CSV_HEADER_VOTE_STATUS,\n",
    "    QA_CSV_HEADER_TIMESTAMPS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL(Trainer):\n",
    "    def __init__(self, rlhf_config: RLHFConfig):\n",
    "        self._rlhf_config = rlhf_config\n",
    "        self.accelerator = Accelerator()\n",
    "        self.num_proc = self._rlhf_config.num_workers if not self._rlhf_config.streaming else None\n",
    "        set_seed(rlhf_config.seed) ## TODO: how to set seed properly in __init__?\n",
    "\n",
    "        self.ppo_config=PPOConfig(\n",
    "            steps=self._rlhf_config.total_ppo_epochs, ## TODO\n",
    "            model_name=self._rlhf_config.base_model_path,\n",
    "            learning_rate=self._rlhf_config.learning_rate,\n",
    "            batch_size=self._rlhf_config.ppo_batch_size,\n",
    "            mini_batch_size=self._rlhf_config.mini_batch_size,\n",
    "            gradient_accumulation_steps=self._rlhf_config.gradient_accumulation_steps,\n",
    "            optimize_cuda_cache=True,\n",
    "            early_stopping=self._rlhf_config.early_stopping,\n",
    "            target_kl=self._rlhf_config.target_kl,\n",
    "            ppo_epochs=self._rlhf_config.ppo_epochs,\n",
    "            seed=self._rlhf_config.seed,\n",
    "            init_kl_coef=self._rlhf_config.init_kl_coef,\n",
    "            adap_kl_ctrl=self._rlhf_config.adap_kl_ctrl,\n",
    "            # accelerator_kwargs=self._rlhf_config.accelerator_kwargs,\n",
    "            )\n",
    "        \n",
    "        ## Load the reward model and tokenizer and define the reward pipeline\n",
    "        self.reward_tokenizer = self.create_tokenizer(rlhf_config.reward_model_path)\n",
    "        self.reward_dataset=self.create_dataset(self.reward_tokenizer)\n",
    "        self.reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            rlhf_config.reward_model_path, \n",
    "            num_labels=1,\n",
    "            # torch_dtype=torch.bfloat16,\n",
    "            load_in_8bit=True,\n",
    "            device_map={\"\": Accelerator().local_process_index}\n",
    "        )\n",
    "        self.reward_kwargs = {\n",
    "            \"top_k\": None, ## TODO `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
    "            \"function_to_apply\": \"none\",\n",
    "            \"batch_size\": self._rlhf_config.ppo_batch_size,\n",
    "            \"truncation\": True,\n",
    "            \"max_length\": self._rlhf_config.output_max_length\n",
    "        }\n",
    "        self.reward_pipe = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=self.reward_model,\n",
    "            # device_map={\"\": Accelerator().local_process_index},\n",
    "            # model_kwargs={\"load_in_8bit\": True},\n",
    "            model_kwargs=self.reward_kwargs,\n",
    "            tokenizer=self.reward_tokenizer,\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "\n",
    "        \n",
    "        ## Load the base model and tokenizer and define the PPO Trainer for RL\n",
    "        self.base_tokenizer = self.create_tokenizer(rlhf_config.base_model_path)\n",
    "        self.base_dataset=self.create_dataset(self.base_tokenizer)\n",
    "        self.base_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "            rlhf_config.base_model_path,\n",
    "            load_in_8bit=rlhf_config.load_in_8bit,\n",
    "            # is_loaded_in_8bit = True, # TODO TypeError: LlamaForCausalLM.__init__() got an unexpected keyword argument 'is_loaded_in_8bit'\n",
    "            # torch_dtype=torch.float16, \n",
    "            device_map={\"\": Accelerator().local_process_index},\n",
    "            peft_config=rlhf_config.lora_config_rl, \n",
    "        )\n",
    "        self.ppo_trainer = PPOTrainer(\n",
    "            config=self.ppo_config,\n",
    "            model=self.base_model,\n",
    "            ref_model=None,\n",
    "            tokenizer=self.base_tokenizer,\n",
    "            dataset=self.base_dataset,\n",
    "            data_collator=self.data_collator,\n",
    "            # optimizer=optimizer,\n",
    "            # peft_config=lora_config, ## PPOTrainer doesn't support parameter peft_config\n",
    "        )\n",
    "        self.base_kwargs = {\n",
    "            \"top_k\": rlhf_config.top_k,\n",
    "            \"top_p\": rlhf_config.top_p,\n",
    "            \"do_sample\": rlhf_config.do_sample,\n",
    "            \"pad_token_id\": self.base_tokenizer.pad_token_id,\n",
    "            \"eos_token_id\": rlhf_config.eos_token_id,\n",
    "        }\n",
    "\n",
    "\n",
    "    def create_tokenizer(self, model_name):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if getattr(tokenizer, \"pad_token\", None) is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        return tokenizer\n",
    "\n",
    "\n",
    "    def data_collator(self, data):\n",
    "        return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "\n",
    "    def create_dataset(self, tokenizer):\n",
    "        \"\"\"\n",
    "        Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "        customize this function to train the model on its own dataset.\n",
    "        \"\"\"\n",
    "        args = self._rlhf_config\n",
    "        if args.dataset_type == \"local_db\":\n",
    "            qa_database = QuestionAnswerDatabase(db_file=self._rlhf_config.dataset_name)\n",
    "            my_data_pd = qa_database.retrieve_all_question_answers_as_pandas()\n",
    "            my_data_pd = my_data_pd[my_data_pd[QA_CSV_HEADER_VOTE_STATUS]==\"up\"]\n",
    "            my_data_pd = my_data_pd[[QA_CSV_HEADER_ID,\n",
    "                                     QA_CSV_HEADER_QUESTION,\n",
    "                                     QA_CSV_HEADER_ANSWER]]\n",
    "            print(\"My local database has {} samples\".format(my_data_pd.shape[0]))\n",
    "            dataset = Dataset.from_dict(my_data_pd)\n",
    "        elif args.dataset_type == \"local_csv\": ## TODO: test\n",
    "            dataset = load_dataset('csv', data_files=args.dataset_name)\n",
    "            dataset = dataset[args.split] # Convert DatasetDict to Dataset\n",
    "        elif args.dataset_type == \"huggingface\": ## TODO: test\n",
    "            dataset = load_dataset(\n",
    "                args.dataset_name,\n",
    "                data_dir=args.dataset_subset_rl,\n",
    "                split=args.split,\n",
    "                use_auth_token=True,\n",
    "                # num_proc=self.num_proc,\n",
    "                # streaming=args.streaming,\n",
    "            )\n",
    "        ## TODO: if args.split in dataset.columns:\n",
    "            # dataset = dataset[args.split] # Convert DatasetDict to Dataset\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No (supported) data files or dataset script found {args.dataset_type}\")\n",
    "        \n",
    "        # dataset = dataset.train_test_split(test_size=args.train_test_split_ratio, \n",
    "        #                                    seed=args.seed)\n",
    "        # print(f\"Size of the train set: {len(dataset['train'])}. \\\n",
    "        #       Size of the validation set: {len(dataset['test'])}\")\n",
    "        \n",
    "        ## TODO: evaluate on eval\n",
    "        # dataset = dataset.select(range(self._rlhf_config.dataset_subset_rl_train))\n",
    "\n",
    "        def preprocess_function(examples):\n",
    "            new_examples = {\n",
    "                \"query\": [],\n",
    "                \"input_ids\": [],\n",
    "            }\n",
    "            for question in examples[QA_CSV_HEADER_QUESTION]:\n",
    "                query = \"Question: \" + question + \"\\n\\nAnswer: \"\n",
    "                tokenized_question = tokenizer(query, truncation=True)\n",
    "                new_examples[\"query\"].append(query)\n",
    "                new_examples[\"input_ids\"].append(tokenized_question[\"input_ids\"])\n",
    "            return new_examples\n",
    "          \n",
    "        dataset = dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=self.num_proc,\n",
    "            remove_columns=dataset.column_names,\n",
    "        )\n",
    "        dataset = dataset.filter(lambda x: len(x[\"input_ids\"]) < self._rlhf_config.max_seq_length, \n",
    "                       batched=False)\n",
    "        dataset.set_format(type=\"torch\") ## TODO\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "\n",
    "    def _train(self, save_checkpoints_path=None):\n",
    "        ## Initialize accelerator\n",
    "        self.ppo_trainer.dataloader = self.accelerator.prepare(self.ppo_trainer.dataloader)\n",
    "\n",
    "        ## training\n",
    "        for epoch, batch in tqdm(enumerate(self.ppo_trainer.dataloader)):\n",
    "            if epoch >= self._rlhf_config.total_ppo_epochs:\n",
    "                break\n",
    "            ## embed the questions and responses to tensors\n",
    "            question_tensors = batch[\"input_ids\"]\n",
    "            response_tensors = self.ppo_trainer.generate(\n",
    "                question_tensors,\n",
    "                return_prompt=False,\n",
    "                length_sampler=LengthSampler(self._rlhf_config.output_min_length, \n",
    "                                             self._rlhf_config.output_max_length),\n",
    "                **self.base_kwargs,\n",
    "            )\n",
    "            batch[QA_CSV_HEADER_ANSWER] = self.base_tokenizer.batch_decode(response_tensors, \n",
    "                                                                 skip_special_tokens=True)\n",
    "            # compute rewards and run PPO\n",
    "            texts = [q + r for q, r in zip(batch[\"query\"], batch[QA_CSV_HEADER_ANSWER])]\n",
    "            pipe_outputs = self.reward_pipe(texts, **self.reward_kwargs)\n",
    "            rewards = [torch.tensor(output[0][\"score\"] - self._rlhf_config.reward_baseline) \\\n",
    "                       for output in pipe_outputs]\n",
    "            stats = self.ppo_trainer.step(question_tensors, response_tensors, rewards)\n",
    "            self.ppo_trainer.log_stats(stats, batch, rewards)\n",
    "            print(\"stats: {}\\n\\n\\n rewards: {}\\n\\n\\n\".format(stats, rewards))\n",
    "\n",
    "            ## save weights\n",
    "            if self._rlhf_config.save_freq and epoch and \\\n",
    "                epoch % self._rlhf_config.save_freq == 0:\n",
    "                if save_checkpoints_path is None:\n",
    "                    save_checkpoints_path = os.path.join(\n",
    "                        save_checkpoints_path, \"checkpoints\",\n",
    "                        f\"checkpoints_epoch_{epoch}\")\n",
    "                self.ppo_trainer.model.save(save_checkpoints_path) ## TODO: only save adapter\n",
    "    \n",
    "    def train(self, save_checkpoints_path=None, num_processes=1):\n",
    "        notebook_launcher(self._train(save_checkpoints_path=save_checkpoints_path),\n",
    "                          num_processes=num_processes)\n",
    "\n",
    "    def save(self, output_path=None):    \n",
    "        if output_path is None:\n",
    "            output_path = os.path.join(\n",
    "                self._rlhf_config.output_dir, \n",
    "                # \"final_lora_models\",\n",
    "                self._rlhf_config.rl_lora_path)\n",
    "        self.ppo_trainer.save_pretrained(output_path)\n",
    "\n",
    "\n",
    "    def train_and_save(self, output_path=None):\n",
    "        self.train(save_checkpoints_path=output_path)\n",
    "        self.save(output_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/goldmermaid___parquet/goldmermaid--stack_exchange_rank_10k_dataset-37ad370d1ca90505/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/goldmermaid___parquet/goldmermaid--stack_exchange_rank_10k_dataset-37ad370d1ca90505/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-4d037661091d7a9d.arrow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = pykoi.RLHFConfig(base_model_path=\"elinas/llama-7b-hf-transformers-4.29\", # \"meta-llama/Llama-2-7b-hf\", \n",
    "                          dataset_type=\"huggingface\", ## \"local_db\",\n",
    "                          dataset_name=\"goldmermaid/stack_exchange_rank_10k_dataset\", ##\"lvwerra/stack-exchange-paired\", ## \"/home/ubuntu/git/pykoi/example/notebook/qd.db\",\n",
    "                          dataset_subset_rl = \"data\",\n",
    "                          reward_model_path=\"goldmermaid/rlhf_reward_model\",\n",
    "                          save_freq=1,\n",
    "                          ppo_batch_size=32,\n",
    "                          ppo_epochs=1,\n",
    "                          total_ppo_epochs=1,\n",
    "                          output_dir=\"./models/rlhf_step3_rl\"\n",
    "                          )\n",
    "rlhf_step3_rl = RL(config)\n",
    "rlhf_step3_rl.train_and_save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
