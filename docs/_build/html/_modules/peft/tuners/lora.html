<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>peft.tuners.lora &mdash; pykoi 0.0.0 documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/sphinx_highlight.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            pykoi
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../modules.html">pykoi</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">pykoi</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">peft.tuners.lora</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for peft.tuners.lora</h1><div class="highlight"><pre>
<span></span><span class="c1"># coding=utf-8</span>
<span class="c1"># Copyright 2023-present the HuggingFace Inc. team.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">asdict</span><span class="p">,</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span><span class="p">,</span> <span class="n">replace</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">Enum</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">transformers.pytorch_utils</span> <span class="kn">import</span> <span class="n">Conv1D</span>

<span class="kn">from</span> <span class="nn">..import_utils</span> <span class="kn">import</span> <span class="n">is_bnb_4bit_available</span><span class="p">,</span> <span class="n">is_bnb_available</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">CLAMP_QUANTILE</span><span class="p">,</span>
    <span class="n">COMMON_LAYERS_PATTERN</span><span class="p">,</span>
    <span class="n">TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING</span><span class="p">,</span>
    <span class="n">ModulesToSaveWrapper</span><span class="p">,</span>
    <span class="n">PeftConfig</span><span class="p">,</span>
    <span class="n">PeftType</span><span class="p">,</span>
    <span class="n">_freeze_adapter</span><span class="p">,</span>
    <span class="n">_get_submodules</span><span class="p">,</span>
    <span class="n">transpose</span><span class="p">,</span>
<span class="p">)</span>


<span class="k">if</span> <span class="n">is_bnb_available</span><span class="p">():</span>
    <span class="kn">import</span> <span class="nn">bitsandbytes</span> <span class="k">as</span> <span class="nn">bnb</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">LoraConfig</span><span class="p">(</span><span class="n">PeftConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is the configuration class to store the configuration of a [`LoraModel`].</span>

<span class="sd">    Args:</span>
<span class="sd">        r (`int`): Lora attention dimension.</span>
<span class="sd">        target_modules (`Union[List[str],str]`): The names of the modules to apply Lora to.</span>
<span class="sd">        lora_alpha (`int`): The alpha parameter for Lora scaling.</span>
<span class="sd">        lora_dropout (`float`): The dropout probability for Lora layers.</span>
<span class="sd">        fan_in_fan_out (`bool`): Set this to True if the layer to replace stores weight like (fan_in, fan_out).</span>
<span class="sd">        For example, gpt-2 uses `Conv1D` which stores weights like (fan_in, fan_out) and hence this should be set to `True`.:</span>
<span class="sd">        bias (`str`): Bias type for Lora. Can be &#39;none&#39;, &#39;all&#39; or &#39;lora_only&#39;</span>
<span class="sd">        modules_to_save (`List[str]`):List of modules apart from LoRA layers to be set as trainable</span>
<span class="sd">            and saved in the final checkpoint.</span>
<span class="sd">        layers_to_transform (`Union[List[int],int]`):</span>
<span class="sd">            The layer indexes to transform, if this argument is specified, it will apply the LoRA transformations on</span>
<span class="sd">            the layer indexes that are specified in this list. If a single integer is passed, it will apply the LoRA</span>
<span class="sd">            transformations on the layer at this index.</span>
<span class="sd">        layers_pattern (`str`):</span>
<span class="sd">            The layer pattern name, used only if `layers_to_transform` is different from `None` and if the layer</span>
<span class="sd">            pattern is not in the common layers pattern.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">r</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="s2">&quot;Lora attention dimension&quot;</span><span class="p">})</span>
    <span class="n">target_modules</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="s2">&quot;List of module names or regex expression of the module names to replace with Lora.&quot;</span>
            <span class="s2">&quot;For example, [&#39;q&#39;, &#39;v&#39;] or &#39;.*decoder.*(SelfAttention|EncDecAttention).*(q|v)$&#39; &quot;</span>
        <span class="p">},</span>
    <span class="p">)</span>
    <span class="n">lora_alpha</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="s2">&quot;Lora alpha&quot;</span><span class="p">})</span>
    <span class="n">lora_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="s2">&quot;Lora dropout&quot;</span><span class="p">})</span>
    <span class="n">fan_in_fan_out</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="s2">&quot;Set this to True if the layer to replace stores weight like (fan_in, fan_out)&quot;</span><span class="p">},</span>
    <span class="p">)</span>
    <span class="n">bias</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="s2">&quot;Bias type for Lora. Can be &#39;none&#39;, &#39;all&#39; or &#39;lora_only&#39;&quot;</span><span class="p">})</span>
    <span class="n">modules_to_save</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="s2">&quot;List of modules apart from LoRA layers to be set as trainable and saved in the final checkpoint. &quot;</span>
            <span class="s2">&quot;For example, in Sequence Classification or Token Classification tasks, &quot;</span>
            <span class="s2">&quot;the final layer `classifier/score` are randomly initialized and as such need to be trainable and saved.&quot;</span>
        <span class="p">},</span>
    <span class="p">)</span>
    <span class="n">init_lora_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="p">(</span>
                <span class="s2">&quot;Whether to initialize the weights of the Lora layers with their default initialization. Don&#39;t change &quot;</span>
                <span class="s2">&quot;this setting, except if you know exactly what you&#39;re doing.&quot;</span>
            <span class="p">),</span>
        <span class="p">},</span>
    <span class="p">)</span>
    <span class="n">layers_to_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="s2">&quot;The layer indexes to transform, is this argument is specified, PEFT will transform only the layers indexes that are specified inside this list. If a single integer is passed, PEFT will transform only the layer at this index.&quot;</span>
        <span class="p">},</span>
    <span class="p">)</span>
    <span class="n">layers_pattern</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;help&quot;</span><span class="p">:</span> <span class="s2">&quot;The layer pattern name, used only if `layers_to_transform` is different to None and if the layer pattern is not in the common layers pattern.&quot;</span>
        <span class="p">},</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">peft_type</span> <span class="o">=</span> <span class="n">PeftType</span><span class="o">.</span><span class="n">LORA</span>


<span class="k">class</span> <span class="nc">LoraModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates Low Rank Adapter (Lora) model from a pretrained transformers model.</span>

<span class="sd">    Args:</span>
<span class="sd">        model ([`~transformers.PreTrainedModel`]): The model to be adapted.</span>
<span class="sd">        config ([`LoraConfig`]): The configuration of the Lora model.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `torch.nn.Module`: The Lora model.</span>

<span class="sd">    Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from transformers import AutoModelForSeq2SeqLM, LoraConfig</span>
<span class="sd">        &gt;&gt;&gt; from peft import LoraModel, LoraConfig</span>

<span class="sd">        &gt;&gt;&gt; config = LoraConfig(</span>
<span class="sd">        ...     peft_type=&quot;LORA&quot;,</span>
<span class="sd">        ...     task_type=&quot;SEQ_2_SEQ_LM&quot;,</span>
<span class="sd">        ...     r=8,</span>
<span class="sd">        ...     lora_alpha=32,</span>
<span class="sd">        ...     target_modules=[&quot;q&quot;, &quot;v&quot;],</span>
<span class="sd">        ...     lora_dropout=0.01,</span>
<span class="sd">        ... )</span>

<span class="sd">        &gt;&gt;&gt; model = AutoModelForSeq2SeqLM.from_pretrained(&quot;t5-base&quot;)</span>
<span class="sd">        &gt;&gt;&gt; lora_model = LoraModel(config, model)</span>
<span class="sd">        ```</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; import transformers</span>
<span class="sd">        &gt;&gt;&gt; from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_int8_training</span>

<span class="sd">        &gt;&gt;&gt; target_modules = [&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;out_proj&quot;, &quot;fc_in&quot;, &quot;fc_out&quot;, &quot;wte&quot;]</span>
<span class="sd">        &gt;&gt;&gt; config = LoraConfig(</span>
<span class="sd">        ...     r=4, lora_alpha=16, target_modules=target_modules, lora_dropout=0.1, bias=&quot;none&quot;, task_type=&quot;CAUSAL_LM&quot;</span>
<span class="sd">        ... )</span>

<span class="sd">        &gt;&gt;&gt; model = transformers.GPTJForCausalLM.from_pretrained(</span>
<span class="sd">        ...     &quot;kakaobrain/kogpt&quot;,</span>
<span class="sd">        ...     revision=&quot;KoGPT6B-ryan1.5b-float16&quot;,  # or float32 version: revision=KoGPT6B-ryan1.5b</span>
<span class="sd">        ...     pad_token_id=tokenizer.eos_token_id,</span>
<span class="sd">        ...     use_cache=False,</span>
<span class="sd">        ...     device_map={&quot;&quot;: rank},</span>
<span class="sd">        ...     torch_dtype=torch.float16,</span>
<span class="sd">        ...     load_in_8bit=True,</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; model = prepare_model_for_int8_training(model)</span>
<span class="sd">        &gt;&gt;&gt; lora_model = get_peft_model(model, config)</span>
<span class="sd">        ```</span>

<span class="sd">    **Attributes**:</span>
<span class="sd">        - **model** ([`~transformers.PreTrainedModel`]) -- The model to be adapted.</span>
<span class="sd">        - **peft_config** ([`LoraConfig`]): The configuration of the Lora model.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">forward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">])</span>

        <span class="c1"># transformers models have a .config attribute, whose presence is assumed later on</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;config&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;model_type&quot;</span><span class="p">:</span> <span class="s2">&quot;custom&quot;</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">add_adapter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model_config</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;config&quot;</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;model_type&quot;</span><span class="p">:</span> <span class="s2">&quot;custom&quot;</span><span class="p">})</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model_config</span><span class="p">,</span> <span class="s2">&quot;to_dict&quot;</span><span class="p">):</span>
                <span class="n">model_config</span> <span class="o">=</span> <span class="n">model_config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>

            <span class="n">config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_lora_config</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">model_config</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_find_and_replace</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span> <span class="o">!=</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;LoraModel supports only 1 adapter with bias. When using multiple adapters, set bias to &#39;none&#39; for all adapters.&quot;</span>
            <span class="p">)</span>
        <span class="n">mark_only_lora_as_trainable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">:</span>
            <span class="n">_freeze_adapter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_quantization_dependency</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">loaded_in_4bit</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;is_loaded_in_4bit&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">loaded_in_8bit</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;is_loaded_in_8bit&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">loaded_in_4bit</span> <span class="ow">or</span> <span class="n">loaded_in_8bit</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_bnb_available</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
                <span class="s2">&quot;To use Lora with 8-bit or 4-bit quantization, please install the `bitsandbytes` package. &quot;</span>
                <span class="s2">&quot;You can install it with `pip install bitsandbytes`.&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_target_module_exists</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lora_config</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lora_config</span><span class="o">.</span><span class="n">target_modules</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">target_module_found</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">fullmatch</span><span class="p">(</span><span class="n">lora_config</span><span class="o">.</span><span class="n">target_modules</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">target_module_found</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">key</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="n">target_key</span><span class="p">)</span> <span class="k">for</span> <span class="n">target_key</span> <span class="ow">in</span> <span class="n">lora_config</span><span class="o">.</span><span class="n">target_modules</span><span class="p">)</span>
            <span class="n">is_using_layer_indexes</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">lora_config</span><span class="p">,</span> <span class="s2">&quot;layers_to_transform&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="n">layer_indexing_pattern</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">lora_config</span><span class="p">,</span> <span class="s2">&quot;layers_pattern&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">is_using_layer_indexes</span> <span class="ow">and</span> <span class="n">target_module_found</span><span class="p">:</span>
                <span class="n">layers_pattern</span> <span class="o">=</span> <span class="n">COMMON_LAYERS_PATTERN</span> <span class="k">if</span> <span class="n">layer_indexing_pattern</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">layer_indexing_pattern</span>
                <span class="n">layers_pattern</span> <span class="o">=</span> <span class="p">[</span><span class="n">layers_pattern</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layers_pattern</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">layers_pattern</span>

                <span class="k">for</span> <span class="n">pattern</span> <span class="ow">in</span> <span class="n">layers_pattern</span><span class="p">:</span>
                    <span class="n">layer_index</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;.*.</span><span class="si">{</span><span class="n">pattern</span><span class="si">}</span><span class="s2">\.(\d+)\.*&quot;</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">layer_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">layer_index</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">layer_index</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
                        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lora_config</span><span class="o">.</span><span class="n">layers_to_transform</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                            <span class="n">target_module_found</span> <span class="o">=</span> <span class="n">layer_index</span> <span class="o">==</span> <span class="n">lora_config</span><span class="o">.</span><span class="n">layers_to_transform</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">target_module_found</span> <span class="o">=</span> <span class="n">layer_index</span> <span class="ow">in</span> <span class="n">lora_config</span><span class="o">.</span><span class="n">layers_to_transform</span>

                        <span class="k">break</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">target_module_found</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="n">target_module_found</span>

    <span class="k">def</span> <span class="nf">_create_new_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lora_config</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">target</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;r&quot;</span><span class="p">:</span> <span class="n">lora_config</span><span class="o">.</span><span class="n">r</span><span class="p">,</span>
            <span class="s2">&quot;lora_alpha&quot;</span><span class="p">:</span> <span class="n">lora_config</span><span class="o">.</span><span class="n">lora_alpha</span><span class="p">,</span>
            <span class="s2">&quot;lora_dropout&quot;</span><span class="p">:</span> <span class="n">lora_config</span><span class="o">.</span><span class="n">lora_dropout</span><span class="p">,</span>
            <span class="s2">&quot;fan_in_fan_out&quot;</span><span class="p">:</span> <span class="n">lora_config</span><span class="o">.</span><span class="n">fan_in_fan_out</span><span class="p">,</span>
            <span class="s2">&quot;init_lora_weights&quot;</span><span class="p">:</span> <span class="n">lora_config</span><span class="o">.</span><span class="n">init_lora_weights</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="n">loaded_in_4bit</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;is_loaded_in_4bit&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">loaded_in_8bit</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;is_loaded_in_8bit&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">loaded_in_8bit</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">bnb</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear8bitLt</span><span class="p">):</span>
            <span class="n">eightbit_kwargs</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">eightbit_kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;has_fp16_weights&quot;</span><span class="p">:</span> <span class="n">target</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">has_fp16_weights</span><span class="p">,</span>
                    <span class="s2">&quot;memory_efficient_backward&quot;</span><span class="p">:</span> <span class="n">target</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">memory_efficient_backward</span><span class="p">,</span>
                    <span class="s2">&quot;threshold&quot;</span><span class="p">:</span> <span class="n">target</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">threshold</span><span class="p">,</span>
                    <span class="s2">&quot;index&quot;</span><span class="p">:</span> <span class="n">target</span><span class="o">.</span><span class="n">index</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">)</span>
            <span class="n">new_module</span> <span class="o">=</span> <span class="n">Linear8bitLt</span><span class="p">(</span>
                <span class="n">adapter_name</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">eightbit_kwargs</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">loaded_in_4bit</span> <span class="ow">and</span> <span class="n">is_bnb_4bit_available</span><span class="p">()</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">bnb</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear4bit</span><span class="p">):</span>
            <span class="n">fourbit_kwargs</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">fourbit_kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;compute_dtype&quot;</span><span class="p">:</span> <span class="n">target</span><span class="o">.</span><span class="n">compute_dtype</span><span class="p">,</span>
                    <span class="s2">&quot;compress_statistics&quot;</span><span class="p">:</span> <span class="n">target</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">compress_statistics</span><span class="p">,</span>
                    <span class="s2">&quot;quant_type&quot;</span><span class="p">:</span> <span class="n">target</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">quant_type</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">)</span>
            <span class="n">new_module</span> <span class="o">=</span> <span class="n">Linear4bit</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">fourbit_kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
            <span class="n">embedding_kwargs</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">embedding_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;fan_in_fan_out&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">embedding_dim</span>
            <span class="n">new_module</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="o">**</span><span class="n">embedding_kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
            <span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">()[:</span><span class="mi">2</span><span class="p">]</span>
            <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">:]</span>
            <span class="n">stride</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">stride</span>
            <span class="n">padding</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">padding</span>
            <span class="n">new_module</span> <span class="o">=</span> <span class="n">Conv2d</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
                <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">out_features</span>
                <span class="k">if</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;fan_in_fan_out&quot;</span><span class="p">]:</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                        <span class="s2">&quot;fan_in_fan_out is set to True but the target module is `torch.nn.Linear`. &quot;</span>
                        <span class="s2">&quot;Setting fan_in_fan_out to False.&quot;</span>
                    <span class="p">)</span>
                    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;fan_in_fan_out&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lora_config</span><span class="o">.</span><span class="n">fan_in_fan_out</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">Conv1D</span><span class="p">):</span>
                <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">target</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">ds_shape</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="s2">&quot;ds_shape&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="n">target</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span>
                <span class="p">)</span>
                <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;is_target_conv_1d_layer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;fan_in_fan_out&quot;</span><span class="p">]:</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                        <span class="s2">&quot;fan_in_fan_out is set to False but the target module is `Conv1D`. &quot;</span>
                        <span class="s2">&quot;Setting fan_in_fan_out to True.&quot;</span>
                    <span class="p">)</span>
                    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;fan_in_fan_out&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lora_config</span><span class="o">.</span><span class="n">fan_in_fan_out</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Target module </span><span class="si">{</span><span class="n">target</span><span class="si">}</span><span class="s2"> is not supported. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Currently, only `torch.nn.Linear` and `Conv1D` are supported.&quot;</span>
                <span class="p">)</span>
            <span class="n">new_module</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">new_module</span>

    <span class="k">def</span> <span class="nf">_find_and_replace</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">):</span>
        <span class="n">lora_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_quantization_dependency</span><span class="p">()</span>
        <span class="n">is_target_modules_in_base_model</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">key_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">key</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()]</span>

        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">key_list</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_target_module_exists</span><span class="p">(</span><span class="n">lora_config</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
                <span class="k">continue</span>

            <span class="n">is_target_modules_in_base_model</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">parent</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">target_name</span> <span class="o">=</span> <span class="n">_get_submodules</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
                <span class="n">target</span><span class="o">.</span><span class="n">update_layer_conv2d</span><span class="p">(</span>
                    <span class="n">adapter_name</span><span class="p">,</span>
                    <span class="n">lora_config</span><span class="o">.</span><span class="n">r</span><span class="p">,</span>
                    <span class="n">lora_config</span><span class="o">.</span><span class="n">lora_alpha</span><span class="p">,</span>
                    <span class="n">lora_config</span><span class="o">.</span><span class="n">lora_dropout</span><span class="p">,</span>
                    <span class="n">lora_config</span><span class="o">.</span><span class="n">init_lora_weights</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
                <span class="n">target</span><span class="o">.</span><span class="n">update_layer_embedding</span><span class="p">(</span>
                    <span class="n">adapter_name</span><span class="p">,</span>
                    <span class="n">lora_config</span><span class="o">.</span><span class="n">r</span><span class="p">,</span>
                    <span class="n">lora_config</span><span class="o">.</span><span class="n">lora_alpha</span><span class="p">,</span>
                    <span class="n">lora_config</span><span class="o">.</span><span class="n">lora_dropout</span><span class="p">,</span>
                    <span class="n">lora_config</span><span class="o">.</span><span class="n">init_lora_weights</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">):</span>
                <span class="n">target</span><span class="o">.</span><span class="n">update_layer</span><span class="p">(</span>
                    <span class="n">adapter_name</span><span class="p">,</span>
                    <span class="n">lora_config</span><span class="o">.</span><span class="n">r</span><span class="p">,</span>
                    <span class="n">lora_config</span><span class="o">.</span><span class="n">lora_alpha</span><span class="p">,</span>
                    <span class="n">lora_config</span><span class="o">.</span><span class="n">lora_dropout</span><span class="p">,</span>
                    <span class="n">lora_config</span><span class="o">.</span><span class="n">init_lora_weights</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_new_module</span><span class="p">(</span><span class="n">lora_config</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_replace_module</span><span class="p">(</span><span class="n">parent</span><span class="p">,</span> <span class="n">target_name</span><span class="p">,</span> <span class="n">new_module</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_target_modules_in_base_model</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Target modules </span><span class="si">{</span><span class="n">lora_config</span><span class="o">.</span><span class="n">target_modules</span><span class="si">}</span><span class="s2"> not found in the base model. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Please check the target modules and try again.&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_replace_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parent_module</span><span class="p">,</span> <span class="n">child_name</span><span class="p">,</span> <span class="n">new_module</span><span class="p">,</span> <span class="n">old_module</span><span class="p">):</span>
        <span class="nb">setattr</span><span class="p">(</span><span class="n">parent_module</span><span class="p">,</span> <span class="n">child_name</span><span class="p">,</span> <span class="n">new_module</span><span class="p">)</span>
        <span class="n">new_module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">old_module</span><span class="o">.</span><span class="n">weight</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">old_module</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">old_module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">new_module</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">old_module</span><span class="o">.</span><span class="n">bias</span>

        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">old_module</span><span class="p">,</span> <span class="s2">&quot;state&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">new_module</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">old_module</span><span class="o">.</span><span class="n">state</span>
            <span class="n">new_module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">old_module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># dispatch to correct device</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">new_module</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="s2">&quot;lora_&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
                <span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">old_module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">if</span> <span class="s2">&quot;ranknum&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
                <span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">old_module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward missing attributes to the wrapped module.&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__getattr__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>  <span class="c1"># defer to nn.Module&#39;s logic</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_peft_config_as_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inference</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="n">config_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">value</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">Enum</span><span class="p">)</span> <span class="k">else</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">asdict</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
            <span class="k">if</span> <span class="n">inference</span><span class="p">:</span>
                <span class="n">config</span><span class="p">[</span><span class="s2">&quot;inference_mode&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">config_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span>
        <span class="k">return</span> <span class="n">config</span>

    <span class="k">def</span> <span class="nf">_set_adapter_layers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">):</span>
                <span class="n">module</span><span class="o">.</span><span class="n">disable_adapters</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="n">enabled</span> <span class="k">else</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">enable_adapter_layers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_adapter_layers</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">disable_adapter_layers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_adapter_layers</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_adapter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">merged</span><span class="p">:</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Adapter cannot be set when the model is merged. Unmerging the model first.&quot;</span><span class="p">)</span>
                    <span class="n">module</span><span class="o">.</span><span class="n">unmerge</span><span class="p">()</span>
                <span class="n">module</span><span class="o">.</span><span class="n">active_adapter</span> <span class="o">=</span> <span class="n">adapter_name</span>

    <span class="k">def</span> <span class="nf">merge_adapter</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method merges the LoRa layers into the base model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">):</span>
                <span class="n">module</span><span class="o">.</span><span class="n">merge</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">unmerge_adapter</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method unmerges the LoRa layers from the base model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">):</span>
                <span class="n">module</span><span class="o">.</span><span class="n">unmerge</span><span class="p">()</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_prepare_lora_config</span><span class="p">(</span><span class="n">peft_config</span><span class="p">,</span> <span class="n">model_config</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">peft_config</span><span class="o">.</span><span class="n">target_modules</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">model_config</span><span class="p">[</span><span class="s2">&quot;model_type&quot;</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Please specify `target_modules` in `peft_config`&quot;</span><span class="p">)</span>
            <span class="n">peft_config</span><span class="o">.</span><span class="n">target_modules</span> <span class="o">=</span> <span class="n">TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING</span><span class="p">[</span><span class="n">model_config</span><span class="p">[</span><span class="s2">&quot;model_type&quot;</span><span class="p">]]</span>
        <span class="k">return</span> <span class="n">peft_config</span>

    <span class="k">def</span> <span class="nf">_unload_and_optionally_merge</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">merge</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;is_loaded_in_8bit&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;is_loaded_in_4bit&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot merge LORA layers when the model is loaded in 8-bit mode&quot;</span><span class="p">)</span>

        <span class="n">key_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">key</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;lora&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">key</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">key_list</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">parent</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">target_name</span> <span class="o">=</span> <span class="n">_get_submodules</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
                    <span class="n">new_module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">out_features</span><span class="p">)</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
                    <span class="n">new_module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
                        <span class="n">target</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span>
                        <span class="n">target</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span>
                        <span class="n">kernel_size</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
                        <span class="n">stride</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                        <span class="n">padding</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                        <span class="n">dilation</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">bias</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="s2">&quot;is_target_conv_1d_layer&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                        <span class="n">new_module</span> <span class="o">=</span> <span class="n">Conv1D</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">in_features</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">new_module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">merge</span><span class="p">:</span>
                    <span class="n">target</span><span class="o">.</span><span class="n">merge</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_replace_module</span><span class="p">(</span><span class="n">parent</span><span class="p">,</span> <span class="n">target_name</span><span class="p">,</span> <span class="n">new_module</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

            <span class="c1"># save any additional trainable modules part of `modules_to_save`</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">ModulesToSaveWrapper</span><span class="p">):</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="n">parent</span><span class="p">,</span> <span class="n">target_name</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">modules_to_save</span><span class="p">[</span><span class="n">target</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">])</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span>

    <span class="k">def</span> <span class="nf">add_weighted_adapter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapters</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">,</span> <span class="n">combination_type</span><span class="o">=</span><span class="s2">&quot;svd&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method adds a new adapter by merging the given adapters with the given weights.</span>

<span class="sd">        Args:</span>
<span class="sd">            adapters (list): List of adapter names to be merged.</span>
<span class="sd">            weights (list): List of weights for each adapter.</span>
<span class="sd">            adapter_name (str): Name of the new adapter.</span>
<span class="sd">            combination_type (str): Type of merging. Can be one of [`svd`, `linear`]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">adapter_name</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="k">return</span>
        <span class="k">for</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">adapters</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">adapter</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adapter </span><span class="si">{</span><span class="n">adapter</span><span class="si">}</span><span class="s2"> does not exist&quot;</span><span class="p">)</span>

        <span class="c1"># if there is only one adapter, we can only use linear merging</span>
        <span class="n">combination_type</span> <span class="o">=</span> <span class="s2">&quot;linear&quot;</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">adapters</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">combination_type</span>

        <span class="c1"># new rank is the max of all ranks of the adapters</span>
        <span class="n">unique_ranks</span> <span class="o">=</span> <span class="nb">list</span><span class="p">({</span><span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span><span class="o">.</span><span class="n">r</span> <span class="k">for</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">adapters</span><span class="p">})</span>
        <span class="k">if</span> <span class="n">combination_type</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">unique_ranks</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;All adapters must have the same r value when using `linear` combination_type&quot;</span><span class="p">)</span>
            <span class="n">new_rank</span> <span class="o">=</span> <span class="n">unique_ranks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">combination_type</span> <span class="o">==</span> <span class="s2">&quot;svd&quot;</span><span class="p">:</span>
            <span class="n">new_rank</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">unique_ranks</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid combination_type: </span><span class="si">{</span><span class="n">combination_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">replace</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">adapters</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">r</span><span class="o">=</span><span class="n">new_rank</span><span class="p">,</span> <span class="n">lora_alpha</span><span class="o">=</span><span class="n">new_rank</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_find_and_replace</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">)</span>
        <span class="n">mark_only_lora_as_trainable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">_freeze_adapter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">)</span>
        <span class="n">key_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">key</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;lora&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">key</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">key_list</span><span class="p">:</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_get_submodules</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">adapter_name</span> <span class="ow">in</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_A</span><span class="p">:</span>
                    <span class="n">target_lora_A</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_A</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
                    <span class="n">target_lora_B</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_B</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
                <span class="k">elif</span> <span class="n">adapter_name</span> <span class="ow">in</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_embedding_A</span><span class="p">:</span>
                    <span class="n">target_lora_A</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_embedding_A</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span>
                    <span class="n">target_lora_B</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_embedding_B</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span>

                <span class="n">target_lora_A</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">target_lora_A</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="mf">0.0</span>
                <span class="n">target_lora_B</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">target_lora_B</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="mf">0.0</span>
                <span class="k">if</span> <span class="n">combination_type</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">adapter</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">adapters</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
                        <span class="k">if</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_A</span><span class="p">:</span>
                            <span class="n">current_adapter_lora_A</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_A</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
                            <span class="n">current_adapter_lora_B</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_B</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
                        <span class="k">elif</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_embedding_A</span><span class="p">:</span>
                            <span class="n">current_adapter_lora_A</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_embedding_A</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span>
                            <span class="n">current_adapter_lora_B</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">lora_embedding_B</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span>
                        <span class="n">target_lora_A</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="n">current_adapter_lora_A</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">target</span><span class="o">.</span><span class="n">scaling</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span>
                        <span class="n">target_lora_B</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="n">current_adapter_lora_B</span><span class="o">.</span><span class="n">data</span>
                <span class="k">elif</span> <span class="n">combination_type</span> <span class="o">==</span> <span class="s2">&quot;svd&quot;</span><span class="p">:</span>
                    <span class="n">target_lora_A</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">target_lora_B</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_svd_weighted_adapter</span><span class="p">(</span>
                        <span class="n">adapters</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">new_rank</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">target_lora_A</span><span class="p">,</span> <span class="n">target_lora_B</span>
                    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_svd_weighted_adapter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapters</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">new_rank</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">target_lora_A</span><span class="p">,</span> <span class="n">target_lora_B</span><span class="p">):</span>
        <span class="n">delta_weight</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">target</span><span class="o">.</span><span class="n">get_delta_weight</span><span class="p">(</span><span class="n">adapters</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">adapter</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">adapters</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
            <span class="n">delta_weight</span> <span class="o">+=</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">target</span><span class="o">.</span><span class="n">get_delta_weight</span><span class="p">(</span><span class="n">adapter</span><span class="p">)</span>
        <span class="n">conv2d</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">Conv2d</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">conv2d</span><span class="p">:</span>
            <span class="n">conv2d_1x1</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">conv2d_1x1</span><span class="p">:</span>
                <span class="n">delta_weight</span> <span class="o">=</span> <span class="n">delta_weight</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">start_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">delta_weight</span> <span class="o">=</span> <span class="n">delta_weight</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">target</span><span class="o">.</span><span class="n">fan_in_fan_out</span><span class="p">:</span>
            <span class="n">delta_weight</span> <span class="o">=</span> <span class="n">delta_weight</span><span class="o">.</span><span class="n">T</span>

        <span class="c1"># based on https://github.com/kohya-ss/sd-scripts/blob/main/networks/svd_merge_lora.py#L114-L131</span>
        <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Vh</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">delta_weight</span><span class="p">)</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">new_rank</span><span class="p">]</span>
        <span class="n">S</span> <span class="o">=</span> <span class="n">S</span><span class="p">[:</span><span class="n">new_rank</span><span class="p">]</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
        <span class="n">Vh</span> <span class="o">=</span> <span class="n">Vh</span><span class="p">[:</span><span class="n">new_rank</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">U</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">Vh</span><span class="o">.</span><span class="n">flatten</span><span class="p">()])</span>
        <span class="n">hi_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">dist</span><span class="p">,</span> <span class="n">CLAMP_QUANTILE</span><span class="p">)</span>
        <span class="n">low_val</span> <span class="o">=</span> <span class="o">-</span><span class="n">hi_val</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">U</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">low_val</span><span class="p">,</span> <span class="n">hi_val</span><span class="p">)</span>
        <span class="n">Vh</span> <span class="o">=</span> <span class="n">Vh</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">low_val</span><span class="p">,</span> <span class="n">hi_val</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">conv2d</span><span class="p">:</span>
            <span class="n">U</span> <span class="o">=</span> <span class="n">U</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">target_lora_B</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">Vh</span> <span class="o">=</span> <span class="n">Vh</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">target_lora_A</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Vh</span><span class="p">,</span> <span class="n">U</span>

    <span class="k">def</span> <span class="nf">delete_adapter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Deletes an existing adapter.</span>

<span class="sd">        Args:</span>
<span class="sd">            adapter_name (str): Name of the adapter to be deleted.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">adapter_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adapter </span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2"> does not exist&quot;</span><span class="p">)</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span>
        <span class="n">key_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">key</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;lora&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">key</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">key_list</span><span class="p">:</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_get_submodules</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="p">[</span>
                    <span class="s2">&quot;r&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;lora_alpha&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;scaling&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;lora_A&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;lora_B&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;lora_embedding_A&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;lora_embedding_B&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;lora_dropout&quot;</span><span class="p">,</span>
                <span class="p">]:</span>
                    <span class="k">if</span> <span class="n">adapter_name</span> <span class="ow">in</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">attr</span><span class="p">):</span>
                        <span class="nb">getattr</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">target</span><span class="o">.</span><span class="n">active_adapter</span> <span class="o">==</span> <span class="n">adapter_name</span><span class="p">:</span>
                    <span class="n">resetting_active_adapter</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Adapter </span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2"> was active which is now deleted. Setting active adapter to </span><span class="si">{</span><span class="n">resetting_active_adapter</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="p">)</span>
                    <span class="n">target</span><span class="o">.</span><span class="n">active_adapter</span> <span class="o">=</span> <span class="n">resetting_active_adapter</span>

    <span class="k">def</span> <span class="nf">merge_and_unload</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method merges the LoRa layers into the base model. This is needed if someone wants to use the base model</span>
<span class="sd">        as a standalone model.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from transformers import AutoModelForCausalLM</span>
<span class="sd">        &gt;&gt;&gt; from peft import PeftModel</span>

<span class="sd">        &gt;&gt;&gt; base_model = AutoModelForCausalLM.from_pretrained(&quot;tiiuae/falcon-40b&quot;)</span>
<span class="sd">        &gt;&gt;&gt; peft_model_id = &quot;smangrul/falcon-40B-int4-peft-lora-sfttrainer-sample&quot;</span>
<span class="sd">        &gt;&gt;&gt; model = PeftModel.from_pretrained(base_model, peft_model_id)</span>
<span class="sd">        &gt;&gt;&gt; merged_model = model.merge_and_unload()</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unload_and_optionally_merge</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">unload</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets back the base model by removing all the lora modules without merging. This gives back the original base</span>
<span class="sd">        model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unload_and_optionally_merge</span><span class="p">(</span><span class="n">merge</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="c1"># Below code is based on https://github.com/microsoft/LoRA/blob/main/loralib/layers.py</span>
<span class="c1"># and modified to work with PyTorch FSDP</span>


<span class="c1">#  ------------------------------------------------------------------------------------------</span>
<span class="c1">#  Copyright (c) Microsoft Corporation. All rights reserved.</span>
<span class="c1">#  Licensed under the MIT License (MIT). See LICENSE in the repo root for license information.</span>
<span class="c1">#  ------------------------------------------------------------------------------------------</span>


<span class="c1"># had to adapt it for `lora_only` to work</span>
<span class="k">def</span> <span class="nf">mark_only_lora_as_trainable</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">bias</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;none&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="s2">&quot;lora_&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">n</span><span class="p">:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="o">==</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
        <span class="k">return</span>
    <span class="k">elif</span> <span class="n">bias</span> <span class="o">==</span> <span class="s2">&quot;all&quot;</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="s2">&quot;bias&quot;</span> <span class="ow">in</span> <span class="n">n</span><span class="p">:</span>
                <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">elif</span> <span class="n">bias</span> <span class="o">==</span> <span class="s2">&quot;lora_only&quot;</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">m</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>


<span class="k">class</span> <span class="nc">LoraLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">r</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lora_alpha</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lora_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">({})</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">({})</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lora_B</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">({})</span>
        <span class="c1"># For Embedding layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lora_embedding_A</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ParameterDict</span><span class="p">({})</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lora_embedding_B</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ParameterDict</span><span class="p">({})</span>
        <span class="c1"># Mark the weight as unmerged</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">merged</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">disable_adapters</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_features</span> <span class="o">=</span> <span class="n">in_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span> <span class="o">=</span> <span class="n">out_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>

    <span class="k">def</span> <span class="nf">update_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">lora_alpha</span><span class="p">,</span> <span class="n">lora_dropout</span><span class="p">,</span> <span class="n">init_lora_weights</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lora_alpha</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">lora_alpha</span>
        <span class="k">if</span> <span class="n">lora_dropout</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="n">lora_dropout_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">lora_dropout</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lora_dropout_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">lora_dropout</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">({</span><span class="n">adapter_name</span><span class="p">:</span> <span class="n">lora_dropout_layer</span><span class="p">}))</span>
        <span class="c1"># Actual trainable parameters</span>
        <span class="k">if</span> <span class="n">r</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">({</span><span class="n">adapter_name</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)}))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lora_B</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">({</span><span class="n">adapter_name</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)}))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">lora_alpha</span> <span class="o">/</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">init_lora_weights</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reset_lora_parameters</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_layer_conv2d</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">lora_alpha</span><span class="p">,</span> <span class="n">lora_dropout</span><span class="p">,</span> <span class="n">init_lora_weights</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lora_alpha</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">lora_alpha</span>
        <span class="k">if</span> <span class="n">lora_dropout</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="n">lora_dropout_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">lora_dropout</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lora_dropout_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">lora_dropout</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">({</span><span class="n">adapter_name</span><span class="p">:</span> <span class="n">lora_dropout_layer</span><span class="p">}))</span>
        <span class="c1"># Actual trainable parameters</span>
        <span class="k">if</span> <span class="n">r</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">kernel_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">]</span>
            <span class="n">stride</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;stride&quot;</span><span class="p">]</span>
            <span class="n">padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;padding&quot;</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">({</span><span class="n">adapter_name</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)})</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lora_B</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">({</span><span class="n">adapter_name</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)})</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">lora_alpha</span> <span class="o">/</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">init_lora_weights</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reset_lora_parameters</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_layer_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">lora_alpha</span><span class="p">,</span> <span class="n">lora_dropout</span><span class="p">,</span> <span class="n">init_lora_weights</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lora_alpha</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">lora_alpha</span>
        <span class="k">if</span> <span class="n">lora_dropout</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="n">lora_dropout_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">lora_dropout</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lora_dropout_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">lora_dropout</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">({</span><span class="n">adapter_name</span><span class="p">:</span> <span class="n">lora_dropout_layer</span><span class="p">}))</span>
        <span class="c1"># Actual trainable parameters</span>
        <span class="k">if</span> <span class="n">r</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">weight_A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">r</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_features</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">weight_B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="n">r</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lora_embedding_A</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ParameterDict</span><span class="p">({</span><span class="n">adapter_name</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">weight_A</span><span class="p">)}))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lora_embedding_B</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ParameterDict</span><span class="p">({</span><span class="n">adapter_name</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">weight_B</span><span class="p">)}))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">lora_alpha</span> <span class="o">/</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">init_lora_weights</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reset_lora_parameters</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset_lora_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">adapter_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="c1"># initialize A the same way as the default for nn.Linear and B to zero</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_B</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">adapter_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_embedding_A</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="c1"># initialize a the same way as the default for nn.linear and b to zero</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_embedding_A</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">])</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_embedding_B</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">):</span>
    <span class="c1"># Lora implemented in a dense layer</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">r</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">lora_alpha</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">lora_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">fan_in_fan_out</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># Set this to True if the layer to replace stores weight like (fan_in, fan_out)</span>
        <span class="n">is_target_conv_1d_layer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">init_lora_weights</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;init_lora_weights&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">LoraLayer</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="o">=</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">out_features</span><span class="p">)</span>
        <span class="c1"># Freezing the pre-trained weight matrix</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">fan_in_fan_out</span> <span class="o">=</span> <span class="n">fan_in_fan_out</span>
        <span class="k">if</span> <span class="n">fan_in_fan_out</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">T</span>

        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_layer</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">lora_alpha</span><span class="p">,</span> <span class="n">lora_dropout</span><span class="p">,</span> <span class="n">init_lora_weights</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span> <span class="o">=</span> <span class="n">adapter_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_target_conv_1d_layer</span> <span class="o">=</span> <span class="n">is_target_conv_1d_layer</span>

    <span class="k">def</span> <span class="nf">merge</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">merged</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Already merged. Nothing to do.&quot;</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_delta_weight</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">merged</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">unmerge</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">merged</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Already unmerged. Nothing to do.&quot;</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_delta_weight</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">merged</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">get_delta_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapter</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">transpose</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">lora_B</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">fan_in_fan_out</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">previous_dtype</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">fan_in_fan_out</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">disable_adapters</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">merged</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">unmerge</span><span class="p">()</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">fan_in_fan_out</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">merged</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">fan_in_fan_out</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

            <span class="n">result</span> <span class="o">+=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">lora_B</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">](</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">](</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_dropout</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">](</span><span class="n">x</span><span class="p">))</span>
                <span class="p">)</span>
                <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">fan_in_fan_out</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

        <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">previous_dtype</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">result</span>


<span class="k">class</span> <span class="nc">Embedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">):</span>
    <span class="c1"># LoRA implemented in a Embedding layer</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">r</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">lora_alpha</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">lora_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">init_lora_weights</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;init_lora_weights&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

        <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">LoraLayer</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="o">=</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_layer_embedding</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">lora_alpha</span><span class="p">,</span> <span class="n">lora_dropout</span><span class="p">,</span> <span class="n">init_lora_weights</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span> <span class="o">=</span> <span class="n">adapter_name</span>

    <span class="k">def</span> <span class="nf">unmerge</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">merged</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Already unmerged. Nothing to do.&quot;</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_delta_weight</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">merged</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">merge</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">merged</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Already merged. Nothing to do.&quot;</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_delta_weight</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">merged</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">get_delta_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapter</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_embedding_B</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_embedding_A</span><span class="p">[</span><span class="n">adapter</span><span class="p">],</span> <span class="kc">True</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">disable_adapters</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">merged</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">unmerge</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">merged</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">after_A</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span>
                    <span class="n">x</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">lora_embedding_A</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">max_norm</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">norm_type</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">scale_grad_by_freq</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">sparse</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">result</span> <span class="o">+=</span> <span class="p">(</span><span class="n">after_A</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_embedding_B</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">result</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">):</span>
    <span class="c1"># Lora implemented in a conv2d layer</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">r</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">lora_alpha</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">lora_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">init_lora_weights</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;init_lora_weights&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

        <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span>
        <span class="n">LoraLayer</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">in_features</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_features</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># Freezing the pre-trained weight matrix</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_layer_conv2d</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">lora_alpha</span><span class="p">,</span> <span class="n">lora_dropout</span><span class="p">,</span> <span class="n">init_lora_weights</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span> <span class="o">=</span> <span class="n">adapter_name</span>

    <span class="k">def</span> <span class="nf">merge</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">merged</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Already merged. Nothing to do.&quot;</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_delta_weight</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">merged</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">unmerge</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">merged</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Already unmerged. Nothing to do.&quot;</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_delta_weight</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">merged</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">get_delta_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapter</span><span class="p">):</span>
        <span class="c1"># https://github.com/bmaltais/kohya_ss/blob/feb6728762a8f463d15ba936d189d4c3abfaa1ab/networks/lora.py#L117</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
            <span class="c1"># conv2d 1x1</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">lora_B</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># conv2d 3x3</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">lora_B</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                <span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
                <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span><span class="p">[</span><span class="n">adapter</span><span class="p">]</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">previous_dtype</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                <span class="n">bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                <span class="n">dilation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
                <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">disable_adapters</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">merged</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">unmerge</span><span class="p">()</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                <span class="n">bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                <span class="n">dilation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
                <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">merged</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                <span class="n">bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                <span class="n">dilation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
                <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

            <span class="n">result</span> <span class="o">+=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">lora_B</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">](</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">](</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_dropout</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">](</span><span class="n">x</span><span class="p">))</span>
                <span class="p">)</span>
                <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                <span class="n">bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                <span class="n">dilation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
                <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">previous_dtype</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">result</span>


<span class="k">if</span> <span class="n">is_bnb_available</span><span class="p">():</span>

    <span class="k">class</span> <span class="nc">Linear8bitLt</span><span class="p">(</span><span class="n">bnb</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear8bitLt</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">):</span>
        <span class="c1"># Lora implemented in a dense layer</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">in_features</span><span class="p">,</span>
            <span class="n">out_features</span><span class="p">,</span>
            <span class="n">r</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
            <span class="n">lora_alpha</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
            <span class="n">lora_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="n">bnb</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear8bitLt</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
                <span class="bp">self</span><span class="p">,</span>
                <span class="n">in_features</span><span class="p">,</span>
                <span class="n">out_features</span><span class="p">,</span>
                <span class="n">bias</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
                <span class="n">has_fp16_weights</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;has_fp16_weights&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
                <span class="n">memory_efficient_backward</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;memory_efficient_backward&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
                <span class="n">threshold</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;threshold&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span>
                <span class="n">index</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;index&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="n">LoraLayer</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="o">=</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">out_features</span><span class="p">)</span>

            <span class="c1"># Freezing the pre-trained weight matrix</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">init_lora_weights</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;init_lora_weights&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update_layer</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">lora_alpha</span><span class="p">,</span> <span class="n">lora_dropout</span><span class="p">,</span> <span class="n">init_lora_weights</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span> <span class="o">=</span> <span class="n">adapter_name</span>

        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">result</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">disable_adapters</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="k">return</span> <span class="n">result</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_autocast_enabled</span><span class="p">():</span>
                    <span class="n">expected_dtype</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">dtype</span>

                    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
                        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                    <span class="n">output</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">lora_B</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">](</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">](</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_dropout</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">](</span><span class="n">x</span><span class="p">))</span>
                        <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">expected_dtype</span><span class="p">)</span>
                        <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">output</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">lora_B</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">](</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">](</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_dropout</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">](</span><span class="n">x</span><span class="p">))</span>
                        <span class="p">)</span>
                        <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span>
                    <span class="p">)</span>
                <span class="n">result</span> <span class="o">+=</span> <span class="n">output</span>
            <span class="k">return</span> <span class="n">result</span>

    <span class="k">if</span> <span class="n">is_bnb_4bit_available</span><span class="p">():</span>

        <span class="k">class</span> <span class="nc">Linear4bit</span><span class="p">(</span><span class="n">bnb</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear4bit</span><span class="p">,</span> <span class="n">LoraLayer</span><span class="p">):</span>
            <span class="c1"># Lora implemented in a dense layer</span>
            <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
                <span class="bp">self</span><span class="p">,</span>
                <span class="n">adapter_name</span><span class="p">,</span>
                <span class="n">in_features</span><span class="p">,</span>
                <span class="n">out_features</span><span class="p">,</span>
                <span class="n">r</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                <span class="n">lora_alpha</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                <span class="n">lora_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">):</span>
                <span class="n">bnb</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear4bit</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
                    <span class="bp">self</span><span class="p">,</span>
                    <span class="n">in_features</span><span class="p">,</span>
                    <span class="n">out_features</span><span class="p">,</span>
                    <span class="n">bias</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
                    <span class="n">compute_dtype</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;compute_dtype&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
                    <span class="n">compress_statistics</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;compress_statistics&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
                    <span class="n">quant_type</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;quant_type&quot;</span><span class="p">,</span> <span class="s2">&quot;nf4&quot;</span><span class="p">),</span>
                <span class="p">)</span>
                <span class="n">LoraLayer</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="o">=</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">out_features</span><span class="p">)</span>

                <span class="c1"># Freezing the pre-trained weight matrix</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

                <span class="n">init_lora_weights</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;init_lora_weights&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">update_layer</span><span class="p">(</span><span class="n">adapter_name</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">lora_alpha</span><span class="p">,</span> <span class="n">lora_dropout</span><span class="p">,</span> <span class="n">init_lora_weights</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span> <span class="o">=</span> <span class="n">adapter_name</span>

            <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">result</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">disable_adapters</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                    <span class="k">return</span> <span class="n">result</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_autocast_enabled</span><span class="p">():</span>
                        <span class="n">expected_dtype</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">dtype</span>
                        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                        <span class="n">output</span> <span class="o">=</span> <span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">lora_B</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">](</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">](</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_dropout</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">](</span><span class="n">x</span><span class="p">))</span>
                            <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">expected_dtype</span><span class="p">)</span>
                            <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">output</span> <span class="o">=</span> <span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">lora_B</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">](</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">](</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_dropout</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">](</span><span class="n">x</span><span class="p">))</span>
                            <span class="p">)</span>
                            <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">active_adapter</span><span class="p">]</span>
                        <span class="p">)</span>
                    <span class="n">result</span> <span class="o">+=</span> <span class="n">output</span>
                <span class="k">return</span> <span class="n">result</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, CambioML.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>